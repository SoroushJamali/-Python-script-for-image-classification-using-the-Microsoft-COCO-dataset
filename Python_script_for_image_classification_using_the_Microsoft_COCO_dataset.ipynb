{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOkgKumchwKcaZl+X4ewA3W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SoroushJamali/-Python-script-for-image-classification-using-the-Microsoft-COCO-dataset/blob/main/Python_script_for_image_classification_using_the_Microsoft_COCO_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script downloads the Microsoft COCO dataset, extracts it, and trains a convolutional neural network to classify images into the specified animal categories. After training the model, you can use it to make predictions on new animal images.\n",
        "\n",
        "Note that you'll need to replace path/to/image.jpg with the actual path to your image file. Also, you may need to adjust the model architecture and hyperparameters depending on the size and complexity of your dataset."
      ],
      "metadata": {
        "id": "YwPwyBzLxIGP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------------------------------"
      ],
      "metadata": {
        "id": "bwlYWMUSyo6R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**import tensorflow as tf**: TensorFlow is a popular open-source library for building and training machine learning models, particularly deep learning models. It provides various tools and modules for constructing and training neural networks, and can be used for a wide range of applications, such as image and speech recognition, natural language processing, and more.\n",
        "\n",
        "**import numpy as np**: NumPy is a popular Python library for numerical computing. It provides tools for working with arrays, matrices, and other numerical data structures, and can be used for a wide range of mathematical computations.\n",
        "\n",
        "**import os**: The os module provides a way to interact with the operating system in Python. It provides various tools for working with files and directories, and can be used to perform operations such as creating, deleting, renaming, or moving files.\n",
        "\n",
        "**import cv2**: OpenCV is an open-source computer vision library that provides tools for image and video processing. The cv2 module provides an interface to the OpenCV library in Python, and can be used for various tasks such as image filtering, object detection, and more.\n",
        "\n",
        "**from tensorflow import keras**: Keras is a high-level API for building and training machine learning models. It provides a simplified interface for constructing neural networks and other machine learning models, and can be used with various backends such as TensorFlow, Theano, and CNTK. In TensorFlow 2.0 and later versions, Keras is included as a part of the TensorFlow package.\n",
        "\n",
        "**from tensorflow.keras import layers**: The layers module provides various types of layers that can be used to build neural networks, such as convolutional layers, pooling layers, dense layers, and more.\n",
        "\n",
        "**import wget**: wget is a Python module that provides a way to download files from the internet. It provides a simple and convenient interface for downloading files, and can be used for various tasks such as downloading datasets or other resources for machine learning.\n",
        "\n",
        "**import tarfile**: The tarfile module provides tools for working with tar archives in Python. It can be used to extract files from tar archives, create new tar archives, and more. In this code, it is used to extract the downloaded tar archive containing the dataset."
      ],
      "metadata": {
        "id": "-TejCLE2yq-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wget"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvUoWmzJyUD_",
        "outputId": "bb0328ed-3941-46ab-8434-d9bffa8e54f6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9674 sha256=9fabcc991edc9f6734733c38629792762b478370f9ee018604432f85475993fd\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/5f/3e/46cc37c5d698415694d83f607f833f83f0149e49b3af9d0f38\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DfHJOwAQwva3"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import wget\n",
        "import tarfile"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These lines download and extract the Microsoft COCO dataset, which is stored as a compressed ZIP file. The wget module is used to download the dataset from the specified URL, and the tarfile module is used to extract the contents of the ZIP file. The os module is used to remove the ZIP file after it has been extracted."
      ],
      "metadata": {
        "id": "YVANCb5uxXN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and extract the dataset\n",
        "dataset_url = 'http://images.cocodataset.org/zips/train2017.zip'\n",
        "dataset_file = 'train2017.zip'\n",
        "wget.download(dataset_url, dataset_file, bar=wget.bar_adaptive)\n",
        "print('Download complete.')\n",
        "with tarfile.open(dataset_file, 'r') as tar:\n",
        "    tar.extractall()\n",
        "os.remove(dataset_file)\n",
        "print('Extraction complete.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "3aAPkF7cw4mX",
        "outputId": "e655f057-f784-49bd-d35f-821d6fe4cff3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-cde8e11b134d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdataset_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'http://images.cocodataset.org/zips/train2017.zip'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdataset_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'train2017.zip'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mwget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar_adaptive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Download complete.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/wget.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(url, out, bar)\u001b[0m\n\u001b[1;32m    524\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0mbinurl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mtmpfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mulib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinurl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmpfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_filename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moutdir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    270\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0mread\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m                 \u001b[0mtfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m                 \u001b[0mblocknum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mreporthook\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These lines define a list of animal classes to be used for classification. In this example, the animal classes are 'person_riding_horse', 'elephant', 'bear', 'zebra', and 'giraffe'. You can customize this list depending on the specific animal classes you want to classify."
      ],
      "metadata": {
        "id": "I7tQU5xIxaiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the classes of animals\n",
        "class_names = ['person_riding_horse', 'elephant', 'bear', 'zebra', 'giraffe']"
      ],
      "metadata": {
        "id": "Lns5vs5ew7UO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These lines load the dataset from the directory where it was extracted. The keras.preprocessing.image_dataset_from_directory function is used to create two datasets: one for training and one for validation. The batch_size parameter determines the number of images to process at a time, the img_height and img_width parameters determine the size of the input images, and the validation_split parameter specifies the fraction of the data to use for validation."
      ],
      "metadata": {
        "id": "z5IYSnEOxcJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "batch_size = 32\n",
        "img_height = 224\n",
        "img_width = 224\n",
        "train_ds = keras.preprocessing.image_dataset_from_directory(\n",
        "    \"train2017\",\n",
        "    labels=\"inferred\",\n",
        "    label_mode=\"int\",\n",
        "    class_names=class_names,\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=batch_size,\n",
        "    image_size=(img_height, img_width),\n",
        "    shuffle=True,\n",
        "    seed=123,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\"\n",
        ")"
      ],
      "metadata": {
        "id": "iFdzC0Y8w7RX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_ds = keras.preprocessing.image_dataset_from_directory(\n",
        "    \"train2017\",\n",
        "    labels=\"inferred\",\n",
        "    label_mode=\"int\",\n",
        "    class_names=class_names,\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=batch_size,\n",
        "    image_size=(img_height, img_width),\n",
        "    shuffle=True,\n",
        "    seed=123,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\"\n",
        ")"
      ],
      "metadata": {
        "id": "CNWazi_0w7O3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These lines define the architecture of the convolutional neural network (CNN) used for image classification. The Sequential function is used to create a sequence of layers, including three convolutional layers, three max pooling layers, a flatten"
      ],
      "metadata": {
        "id": "Thdq9plGxg7H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "model = keras.Sequential([...]): This creates a Sequential model, which is a linear stack of layers. The layers are defined inside the list passed as an argument to the Sequential function.\n",
        "\n",
        "layers.experimental.preprocessing.Rescaling(1./255): This rescales the input images so that the pixel values are between 0 and 1, which makes it easier for the model to learn from the data.\n",
        "\n",
        "layers.Conv2D(32, 3, activation='relu'): This adds a convolutional layer with 32 filters and a 3x3 kernel size, with ReLU activation.\n",
        "\n",
        "layers.MaxPooling2D(): This adds a max pooling layer to downsample the output of the previous layer.\n",
        "\n",
        "The next two lines add two more convolutional layers with 32 filters and 3x3 kernel size each, followed by max pooling layers.\n",
        "\n",
        "layers.Flatten(): This flattens the output of the previous layer into a 1D array, which is then passed to a fully connected layer.\n",
        "\n",
        "layers.Dense(128, activation='relu'): This adds a fully connected layer with 128 units and ReLU activation.\n",
        "\n",
        "layers.Dense(len(class_names)): This adds the output layer with the number of units equal to the number of classes, which is determined by the length of the class_names list.\n",
        "\n",
        "model.compile(...): This compiles the model, specifying the optimizer, loss function, and metrics to be used during training. In this case, the Adam optimizer is used with Sparse Categorical Crossentropy loss and accuracy as the evaluation metric. The from_logits=True parameter indicates that the output of the model is not normalized, and needs to be passed through a softmax function during training."
      ],
      "metadata": {
        "id": "4nBGrEmyx8In"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model architecture\n",
        "model = keras.Sequential([\n",
        "    layers.experimental.preprocessing.Rescaling(1./255),\n",
        "    layers.Conv2D(32, 3, activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Conv2D(32, 3, activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Conv2D(64, 3, activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(len(class_names))\n",
        "])"
      ],
      "metadata": {
        "id": "8ZSDxAE_w7MP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "ydtdYF7Hw7Jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "epochs = 10\n",
        "model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs\n",
        ")"
      ],
      "metadata": {
        "id": "V_T9xdWJxCVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on new data\n",
        "img_path = \"path/to/image.jpg\"\n",
        "img = keras.preprocessing.image.load_img(\n",
        "    img_path, target_size=(img_height, img_width)\n",
        ")\n",
        "img_array = keras.preprocessing.image.img_to_array(img)\n",
        "img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
        "\n",
        "predictions = model.predict(img_array)\n",
        "score = tf.nn.softmax(predictions[0])\n",
        "\n",
        "print(\"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
        "      .format(class_names[np.argmax(score)], 100 * np.max(score)))"
      ],
      "metadata": {
        "id": "7u6tyUEoxCTc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}